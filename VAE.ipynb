{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+7IDT7C10JhNVj1hU/ulu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenlaw/VAES/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dBh50i3tGwYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeUKnhL4Gt4V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "需要的python包"
      ],
      "metadata": {
        "id": "sGFEQ9ZXGucv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QHjPrnTwG1Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.default_device = 'cuda'\n",
        "else:\n",
        "    torch.default_device = 'cpu'"
      ],
      "metadata": {
        "id": "7Wr1i-cfYlWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据和默认参数"
      ],
      "metadata": {
        "id": "vV8zQ6hpG7rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dataset =torchvision.datasets.MNIST(\".\",transform=ToTensor(),download=True)\n",
        "dataLoader = DataLoader(\n",
        "    mnist_dataset,\n",
        "    batch_size=100,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "#默认参数\n",
        "latent_dim = 2\n",
        "original_dim =28*28\n",
        "immediate_dim = 256\n",
        "epoch = 50"
      ],
      "metadata": {
        "id": "zv77ItpdHAsF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33dfa44-d217-4a22-8bb3-292ce66ffbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 479kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.45MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.84MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE模型"
      ],
      "metadata": {
        "id": "D-lsh1vcHFI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(original_dim, immediate_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.medium = nn.Linear(immediate_dim, latent_dim)\n",
        "        self.log_var = nn.Linear(immediate_dim, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, immediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(immediate_dim, original_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        return torch.randn(n_samples, latent_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        mid = self.encoder(input)\n",
        "        med = self.medium(mid)\n",
        "        var = self.log_var(mid)\n",
        "        std = torch.exp(0.5 * var) + 1e-8  # 修正epsilon位置\n",
        "        u = self.sample(input.size(0)).to(device) * std + med  # 动态batch size适配\n",
        "        return self.decoder(u), med, var\n",
        "\n",
        "    def generate(self, z):\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "BVYPkQQ9HGft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss(reduction='sum')\n",
        "\n",
        "\n",
        "def loss_function(input, output, med, var):\n",
        "    # 调整输入输出维度适配\n",
        "    x_loss = criterion(output, input.view(-1, original_dim))\n",
        "\n",
        "    # 计算KL散度（均值代替求和）\n",
        "    KL_loss = -0.5 * torch.sum(1 + var - med.pow(2) - var.exp())\n",
        "\n",
        "    return (x_loss + KL_loss)/2  # 移除人工缩放"
      ],
      "metadata": {
        "id": "Tys-B8jxHOig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for batch, (X, _) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        X=X.to(device)\n",
        "\n",
        "        # 前向传播\n",
        "        recon, med, var = model(X)\n",
        "\n",
        "        # 计算损失\n",
        "        loss = loss_fn(X, recon, med, var)\n",
        "\n",
        "        # 反向传播\n",
        "        loss.backward()\n",
        "\n",
        "        # 梯度裁剪防止爆炸\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # 参数更新\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{len(dataloader.dataset):>5d}]\")\n"
      ],
      "metadata": {
        "id": "XfCEsJEWHRMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # 改用Adam优化器\n",
        "\n",
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train_loop(dataLoader, model, loss_function, optimizer)\n",
        "print(\"Done!\")\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), \"vae_mnist.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tp4D0mFYaAE",
        "outputId": "ce4aebfa-5b92-4ae8-d4f3-fc287a6ce851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 27558.556641  [    0/60000]\n",
            "loss: 9756.928711  [10000/60000]\n",
            "loss: 9173.337891  [20000/60000]\n",
            "loss: 8731.300781  [30000/60000]\n",
            "loss: 8758.790039  [40000/60000]\n",
            "loss: 8677.389648  [50000/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 8603.009766  [    0/60000]\n",
            "loss: 8611.987305  [10000/60000]\n",
            "loss: 8383.133789  [20000/60000]\n",
            "loss: 8627.634766  [30000/60000]\n",
            "loss: 8156.881348  [40000/60000]\n",
            "loss: 8104.748535  [50000/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 8339.137695  [    0/60000]\n",
            "loss: 8138.273438  [10000/60000]\n",
            "loss: 7932.479004  [20000/60000]\n",
            "loss: 7839.581055  [30000/60000]\n",
            "loss: 8589.541992  [40000/60000]\n",
            "loss: 8136.560059  [50000/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 8138.820312  [    0/60000]\n",
            "loss: 7773.876465  [10000/60000]\n",
            "loss: 8119.705078  [20000/60000]\n",
            "loss: 8211.324219  [30000/60000]\n",
            "loss: 8028.455566  [40000/60000]\n",
            "loss: 8027.893555  [50000/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 7527.189941  [    0/60000]\n",
            "loss: 8510.269531  [10000/60000]\n",
            "loss: 7983.766602  [20000/60000]\n",
            "loss: 7894.260742  [30000/60000]\n",
            "loss: 7738.395996  [40000/60000]\n",
            "loss: 8013.035645  [50000/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 7942.732422  [    0/60000]\n",
            "loss: 7855.511230  [10000/60000]\n",
            "loss: 7788.180664  [20000/60000]\n",
            "loss: 8184.996094  [30000/60000]\n",
            "loss: 7979.748047  [40000/60000]\n",
            "loss: 8244.185547  [50000/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 7874.591309  [    0/60000]\n",
            "loss: 7762.429199  [10000/60000]\n",
            "loss: 7728.609863  [20000/60000]\n",
            "loss: 7942.630859  [30000/60000]\n",
            "loss: 8106.499023  [40000/60000]\n",
            "loss: 8310.493164  [50000/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 7881.961426  [    0/60000]\n",
            "loss: 7819.584961  [10000/60000]\n",
            "loss: 7784.921387  [20000/60000]\n",
            "loss: 7450.208496  [30000/60000]\n",
            "loss: 7766.437988  [40000/60000]\n",
            "loss: 8014.644043  [50000/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 7855.785645  [    0/60000]\n",
            "loss: 7904.909180  [10000/60000]\n",
            "loss: 7673.944824  [20000/60000]\n",
            "loss: 7790.344727  [30000/60000]\n",
            "loss: 7975.291016  [40000/60000]\n",
            "loss: 7711.806152  [50000/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 7548.317383  [    0/60000]\n",
            "loss: 7506.583984  [10000/60000]\n",
            "loss: 7646.085938  [20000/60000]\n",
            "loss: 7955.598633  [30000/60000]\n",
            "loss: 7603.849609  [40000/60000]\n",
            "loss: 7315.192871  [50000/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 7078.599121  [    0/60000]\n",
            "loss: 7358.642090  [10000/60000]\n",
            "loss: 7921.624512  [20000/60000]\n",
            "loss: 8393.098633  [30000/60000]\n",
            "loss: 7988.914551  [40000/60000]\n",
            "loss: 7903.832031  [50000/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 8242.980469  [    0/60000]\n",
            "loss: 7496.995605  [10000/60000]\n",
            "loss: 8006.829590  [20000/60000]\n",
            "loss: 7719.926758  [30000/60000]\n",
            "loss: 7747.896973  [40000/60000]\n",
            "loss: 7663.739746  [50000/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 7973.537109  [    0/60000]\n",
            "loss: 7662.715332  [10000/60000]\n",
            "loss: 7618.804199  [20000/60000]\n",
            "loss: 7951.778320  [30000/60000]\n",
            "loss: 8004.837891  [40000/60000]\n",
            "loss: 7475.344238  [50000/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 7498.522461  [    0/60000]\n",
            "loss: 7611.445312  [10000/60000]\n",
            "loss: 8133.739258  [20000/60000]\n",
            "loss: 7562.615723  [30000/60000]\n",
            "loss: 7636.583984  [40000/60000]\n",
            "loss: 7435.769043  [50000/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 7583.616211  [    0/60000]\n",
            "loss: 7891.438477  [10000/60000]\n",
            "loss: 7467.271484  [20000/60000]\n",
            "loss: 7778.018555  [30000/60000]\n",
            "loss: 7212.017578  [40000/60000]\n",
            "loss: 8065.111328  [50000/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 7919.197266  [    0/60000]\n",
            "loss: 7677.150391  [10000/60000]\n",
            "loss: 7801.222168  [20000/60000]\n",
            "loss: 7476.217773  [30000/60000]\n",
            "loss: 7449.868652  [40000/60000]\n",
            "loss: 7670.793457  [50000/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 7438.999512  [    0/60000]\n",
            "loss: 7406.262207  [10000/60000]\n",
            "loss: 7488.569824  [20000/60000]\n",
            "loss: 7899.153320  [30000/60000]\n",
            "loss: 7766.732910  [40000/60000]\n",
            "loss: 7860.696289  [50000/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 7547.062988  [    0/60000]\n",
            "loss: 7707.716309  [10000/60000]\n",
            "loss: 7975.743164  [20000/60000]\n",
            "loss: 7400.362793  [30000/60000]\n",
            "loss: 7207.501465  [40000/60000]\n",
            "loss: 7322.886719  [50000/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 7130.569824  [    0/60000]\n",
            "loss: 7355.707031  [10000/60000]\n",
            "loss: 7926.327637  [20000/60000]\n",
            "loss: 6958.161621  [30000/60000]\n",
            "loss: 7638.096191  [40000/60000]\n",
            "loss: 7519.716797  [50000/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 7348.505859  [    0/60000]\n",
            "loss: 7636.519043  [10000/60000]\n",
            "loss: 7679.837402  [20000/60000]\n",
            "loss: 7451.479004  [30000/60000]\n",
            "loss: 7413.448242  [40000/60000]\n",
            "loss: 7556.360840  [50000/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 7988.442383  [    0/60000]\n",
            "loss: 7641.791992  [10000/60000]\n",
            "loss: 7625.723633  [20000/60000]\n",
            "loss: 7171.795410  [30000/60000]\n",
            "loss: 7268.137207  [40000/60000]\n",
            "loss: 7511.947266  [50000/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 7462.111328  [    0/60000]\n",
            "loss: 7524.469238  [10000/60000]\n",
            "loss: 7942.790039  [20000/60000]\n",
            "loss: 7325.641113  [30000/60000]\n",
            "loss: 7509.543457  [40000/60000]\n",
            "loss: 7541.844727  [50000/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 7626.874023  [    0/60000]\n",
            "loss: 7807.831543  [10000/60000]\n",
            "loss: 7313.653809  [20000/60000]\n",
            "loss: 7503.610352  [30000/60000]\n",
            "loss: 7662.006348  [40000/60000]\n",
            "loss: 7417.714355  [50000/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 7602.783203  [    0/60000]\n",
            "loss: 7555.801758  [10000/60000]\n",
            "loss: 7828.315430  [20000/60000]\n",
            "loss: 7689.451660  [30000/60000]\n",
            "loss: 7307.906250  [40000/60000]\n",
            "loss: 7258.109863  [50000/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 7316.980957  [    0/60000]\n",
            "loss: 7787.094727  [10000/60000]\n",
            "loss: 7341.326172  [20000/60000]\n",
            "loss: 7759.734863  [30000/60000]\n",
            "loss: 7313.914551  [40000/60000]\n",
            "loss: 7523.182129  [50000/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 7304.595215  [    0/60000]\n",
            "loss: 7530.510254  [10000/60000]\n",
            "loss: 7983.720215  [20000/60000]\n",
            "loss: 7399.639160  [30000/60000]\n",
            "loss: 7509.330566  [40000/60000]\n",
            "loss: 7664.534668  [50000/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 6941.487793  [    0/60000]\n",
            "loss: 7511.479492  [10000/60000]\n",
            "loss: 7520.065430  [20000/60000]\n",
            "loss: 7884.970703  [30000/60000]\n",
            "loss: 7468.102539  [40000/60000]\n",
            "loss: 7242.420898  [50000/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 7694.538574  [    0/60000]\n",
            "loss: 6979.799316  [10000/60000]\n",
            "loss: 7835.091309  [20000/60000]\n",
            "loss: 7290.179688  [30000/60000]\n",
            "loss: 7864.335449  [40000/60000]\n",
            "loss: 7252.928223  [50000/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 7500.886230  [    0/60000]\n",
            "loss: 7164.456055  [10000/60000]\n",
            "loss: 7565.973145  [20000/60000]\n",
            "loss: 7709.097656  [30000/60000]\n",
            "loss: 7673.075195  [40000/60000]\n",
            "loss: 7608.676758  [50000/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 7045.775879  [    0/60000]\n",
            "loss: 7409.701172  [10000/60000]\n",
            "loss: 7266.525391  [20000/60000]\n",
            "loss: 7361.975586  [30000/60000]\n",
            "loss: 7583.448730  [40000/60000]\n",
            "loss: 7562.471191  [50000/60000]\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 7337.516602  [    0/60000]\n",
            "loss: 7547.925293  [10000/60000]\n",
            "loss: 7309.687500  [20000/60000]\n",
            "loss: 7500.736328  [30000/60000]\n",
            "loss: 7079.232910  [40000/60000]\n",
            "loss: 7617.134277  [50000/60000]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 7304.115234  [    0/60000]\n",
            "loss: 7544.815918  [10000/60000]\n",
            "loss: 7501.678711  [20000/60000]\n",
            "loss: 7589.708984  [30000/60000]\n",
            "loss: 7692.932129  [40000/60000]\n",
            "loss: 7412.750977  [50000/60000]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 7424.302734  [    0/60000]\n",
            "loss: 7665.343750  [10000/60000]\n",
            "loss: 7172.963867  [20000/60000]\n",
            "loss: 7264.604004  [30000/60000]\n",
            "loss: 7224.462891  [40000/60000]\n",
            "loss: 7347.251465  [50000/60000]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 7157.449219  [    0/60000]\n",
            "loss: 7215.232910  [10000/60000]\n",
            "loss: 7434.390625  [20000/60000]\n",
            "loss: 7327.341797  [30000/60000]\n",
            "loss: 7286.723633  [40000/60000]\n",
            "loss: 7406.989746  [50000/60000]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 7688.279785  [    0/60000]\n",
            "loss: 7743.517578  [10000/60000]\n",
            "loss: 7546.031250  [20000/60000]\n",
            "loss: 7792.824219  [30000/60000]\n",
            "loss: 7086.058594  [40000/60000]\n",
            "loss: 7644.274902  [50000/60000]\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 7191.638672  [    0/60000]\n",
            "loss: 7337.234863  [10000/60000]\n",
            "loss: 7375.314941  [20000/60000]\n",
            "loss: 7570.725098  [30000/60000]\n",
            "loss: 7553.621094  [40000/60000]\n",
            "loss: 7789.406738  [50000/60000]\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 7719.322754  [    0/60000]\n",
            "loss: 7682.308105  [10000/60000]\n",
            "loss: 7031.274902  [20000/60000]\n",
            "loss: 6916.968750  [30000/60000]\n",
            "loss: 7450.285645  [40000/60000]\n",
            "loss: 7683.984863  [50000/60000]\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 7745.809570  [    0/60000]\n",
            "loss: 7151.217773  [10000/60000]\n",
            "loss: 7082.531250  [20000/60000]\n",
            "loss: 7752.434570  [30000/60000]\n",
            "loss: 7329.249512  [40000/60000]\n",
            "loss: 7058.123535  [50000/60000]\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 7170.196289  [    0/60000]\n",
            "loss: 7677.510254  [10000/60000]\n",
            "loss: 7027.273438  [20000/60000]\n",
            "loss: 7363.095703  [30000/60000]\n",
            "loss: 7420.896973  [40000/60000]\n",
            "loss: 7829.754883  [50000/60000]\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 6942.306641  [    0/60000]\n",
            "loss: 7359.108887  [10000/60000]\n",
            "loss: 7611.014160  [20000/60000]\n",
            "loss: 7143.041992  [30000/60000]\n",
            "loss: 7612.660156  [40000/60000]\n",
            "loss: 7680.670410  [50000/60000]\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 7189.042480  [    0/60000]\n",
            "loss: 7254.781250  [10000/60000]\n",
            "loss: 7644.397461  [20000/60000]\n",
            "loss: 7349.450195  [30000/60000]\n",
            "loss: 7434.721191  [40000/60000]\n",
            "loss: 7228.347656  [50000/60000]\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 7524.039062  [    0/60000]\n",
            "loss: 7717.284668  [10000/60000]\n",
            "loss: 7291.311035  [20000/60000]\n",
            "loss: 7191.944336  [30000/60000]\n",
            "loss: 7355.219727  [40000/60000]\n",
            "loss: 7118.609863  [50000/60000]\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 7220.915039  [    0/60000]\n",
            "loss: 7203.728027  [10000/60000]\n",
            "loss: 7383.456543  [20000/60000]\n",
            "loss: 7498.934082  [30000/60000]\n",
            "loss: 7458.352051  [40000/60000]\n",
            "loss: 7273.060059  [50000/60000]\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 7569.063477  [    0/60000]\n",
            "loss: 7307.453613  [10000/60000]\n",
            "loss: 7327.657715  [20000/60000]\n",
            "loss: 7480.208496  [30000/60000]\n",
            "loss: 7757.001465  [40000/60000]\n",
            "loss: 7564.293945  [50000/60000]\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 7635.197266  [    0/60000]\n",
            "loss: 7039.211426  [10000/60000]\n",
            "loss: 6999.245117  [20000/60000]\n",
            "loss: 7564.333984  [30000/60000]\n",
            "loss: 7239.989746  [40000/60000]\n",
            "loss: 7720.895996  [50000/60000]\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 7018.535156  [    0/60000]\n",
            "loss: 7513.222656  [10000/60000]\n",
            "loss: 7368.186035  [20000/60000]\n",
            "loss: 7095.994141  [30000/60000]\n",
            "loss: 7574.254395  [40000/60000]\n",
            "loss: 7514.588379  [50000/60000]\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 7185.503906  [    0/60000]\n",
            "loss: 7131.228027  [10000/60000]\n",
            "loss: 7208.850098  [20000/60000]\n",
            "loss: 7185.331055  [30000/60000]\n",
            "loss: 7181.648926  [40000/60000]\n",
            "loss: 7345.349609  [50000/60000]\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 7008.885254  [    0/60000]\n",
            "loss: 7308.314453  [10000/60000]\n",
            "loss: 7167.762207  [20000/60000]\n",
            "loss: 7343.931641  [30000/60000]\n",
            "loss: 7385.079590  [40000/60000]\n",
            "loss: 7002.875977  [50000/60000]\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 7283.204102  [    0/60000]\n",
            "loss: 7252.864746  [10000/60000]\n",
            "loss: 7441.420898  [20000/60000]\n",
            "loss: 7384.871094  [30000/60000]\n",
            "loss: 7334.997070  [40000/60000]\n",
            "loss: 7257.465820  [50000/60000]\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 7107.813965  [    0/60000]\n",
            "loss: 7662.825195  [10000/60000]\n",
            "loss: 7456.327637  [20000/60000]\n",
            "loss: 7361.499512  [30000/60000]\n",
            "loss: 7181.639160  [40000/60000]\n",
            "loss: 7360.598633  [50000/60000]\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    pic =model.generate(model.sample(1).to(device)).cpu()\n",
        "\n",
        "# 可视化\n",
        "    plt.imshow(pic.reshape(28,-1))\n",
        "    plt.axis('off')  # 不显示坐标轴\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "NOJ3y8wyHT1U",
        "outputId": "fe5c4c6e-3568-4a00-94d6-a518bd44d7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}